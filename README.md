# ğŸ–¼ï¸ DB6 â€“ Dual Function Image System for Multimodal Interface



## ğŸ‘¥ Team Info

- 22471A05O7 â€” **Y. Krupa Chaitanya** ( [@krupa1030](www.linkedin.com/in/krupa1030) )  
  _Work Done: Designed the overall system architecture and implemented the CLIP-based semantic embedding module. Also integrated image retrieval and text-to-image generation into a unified pipeline._

- 22471A05N8 â€” **Sk. A. Abdul Kareem** ( [@abdul](https://www.linkedin.com/in/shaik-anamtaram-abdul-kareem-301114288/) )  
  _Work Done: Performed dataset collection, cleaning, and preprocessing for WANG and ImageCLEF datasets. Conducted model evaluation using accuracy, precision, recall, and F1-score metrics._

- 23475A0507 â€” **G. L. Vara Prasad** ( [@prasad](https://www.linkedin.com/in/gogineniprasadchowdary/) )  
  _Work Done: Implemented YOLOv8 for object detection and region extraction from images. Optimized FAISS indexing for fast similarity search and reduced query response time._

---

## ğŸ“Œ Abstract
This project presents a dual-function image system that performs both **semantic image retrieval** and **text-to-image generation** in a unified framework. It uses **CLIP (ViT-B/32)** to extract semantic embeddings, **YOLOv8** for object detection, **FAISS** for fast similarity search, and **Stable Diffusion v1.5** for generating images from text prompts. The system is evaluated on **WANG** and **ImageCLEF** datasets and achieves high accuracy, recall, and low query time.

---

## ğŸ“„ Paper Reference (Inspiration)
ğŸ‘‰ **[Improving the Efficiency of Semantic Image Retrieval Using a Combined Graph and SOM Model â€“ Nguyen Minh Hai et al.](https://ieeexplore.ieee.org/document/10289012)**  

---

## ğŸš€ Our Improvement Over Existing Paper
- âŒ Removes static ontology dependency: Unlike the existing paper which relies on manually built ontologies and static semantic structures, this project eliminates the need for predefined knowledge graphs and RDF-based representations, making the system more flexible and easier to scale.
- âš¡ Faster retrieval using FAISS: The system leverages Facebook AI Similarity Search (FAISS) for high-speed vector indexing and similarity matching, significantly reducing query response time compared to traditional graph traversal methods.
- ğŸ§  Uses deep semantic embeddings (CLIP): Semantic understanding is improved by using CLIP (ViT-B/32), which maps both images and text into a shared embedding space, enabling accurate cross-modal and semantic similarity search.
- ğŸ–¼ï¸ Adds **Text-to-Image Generation**: In addition to image retrieval, the system supports text-to-image generation using Stable Diffusion v1.5, allowing users to synthesize realistic images from natural language prompts, which was not supported in the original model.
- ğŸ”„ Supports zero-shot learning: The system can retrieve and classify unseen categories without retraining by using CLIPâ€™s zero-shot capability, increasing generalization and real-world applicability.

---

## ğŸ§© About the Project
This project implements a dual-function multimodal system capable of performing semantic image retrieval and text-to-image generation within a single framework. Users can upload an image to retrieve visually and semantically similar images from the dataset or provide a text prompt to generate a new image.  
The system is useful for applications such as educational content discovery, digital media design, e-commerce product search, and visual surveillance.

### ğŸ” Workflow
**Input Image / Text â†’ Preprocessing â†’ YOLOv8 (Object Detection) â†’ CLIP (Feature Extraction) â†’ FAISS (Search) â†’ Output Image / Generated Image**  
- Input is taken either as an image or text.  
- Images are preprocessed and semantic regions are detected using YOLOv8.  
- CLIP extracts semantic embeddings from detected regions.  
- FAISS performs similarity search on indexed embeddings.  
- Output is returned as similar images or newly generated images.

---

## ğŸ“Š Dataset Used
ğŸ‘‰ **[WANG Dataset](https://www.kaggle.com/datasets/elkamel/corel-image-dataset)**  
ğŸ‘‰ **[ImageCLEF Dataset](https://www.imageclef.org/)**  

### ğŸ—‚ Dataset Details
- ğŸŸ¢ **WANG Dataset**: Contains 10,000 natural images divided into 80 semantic classes with 100 images per class. It provides a clean and balanced benchmark for evaluating classification and retrieval accuracy.
- ğŸ”µ **ImageCLEF Dataset**: Contains more than 20,000 images across 276 fine-grained categories, representing complex real-world scenes. It is used to evaluate system robustness and generalization.

---

## ğŸ§° Dependencies Used
- ğŸ **Python** â€“ Core programming language used for system development  
- ğŸ‘ï¸ **OpenCV** â€“ Image loading, resizing, and preprocessing  
- ğŸ”¥ **PyTorch** â€“ Deep learning framework for CLIP, YOLOv8, and Stable Diffusion  
- ğŸ“Š **NumPy** â€“ Numerical computation and matrix operations  
- ğŸ“ˆ **scikit-learn** â€“ Performance evaluation metrics  
- ğŸ§  **CLIP** â€“ Semantic embedding generation for image and text  
- ğŸ“¦ **YOLOv8** â€“ Object detection and region extraction  
- âš¡ **FAISS** â€“ Fast vector similarity search  
- ğŸ¨ **Stable Diffusion** â€“ Text-to-image generation  
- ğŸ“‰ **Matplotlib** â€“ Visualization of results

---

## ğŸ” EDA & Preprocessing
- ğŸ–¼ï¸ All images are converted to RGB format to maintain uniformity.  
- ğŸ“ Images are resized to 512Ã—512 pixels with aspect ratio preservation to ensure model compatibility.  
- ğŸ§¹ Corrupted and unsupported image files are removed during data cleaning.  
- ğŸ·ï¸ Class labels are automatically extracted from directory names and file names to reduce manual annotation effort.  
- âœ‚ YOLOv8 is used to detect and crop semantic regions, enabling object-level feature extraction instead of full-image features.

---

## ğŸ§ª Model Training Info
- ğŸ§  CLIP (ViT-B/32) generates 512-dimensional semantic embeddings for both images and text.  
- ğŸ¯ YOLOv8 detects objects and extracts meaningful regions of interest.  
- âš¡ FAISS indexes the embeddings and performs nearest-neighbor similarity search efficiently.  
- ğŸ¨ Stable Diffusion v1.5 generates high-resolution images from text prompts using a latent diffusion process.

---

## ğŸ§¾ Model Testing / Evaluation
ğŸ“ **Metrics Used:**  
- Accuracy  
- Precision  
- Recall  
- F1-score  
- ROC-AUC  

ğŸ†š **Compared With:**  
- GP-Tree  
- Graph-GPTree  
- SgGP-Tree  

Performance is evaluated on WANG and ImageCLEF datasets and benchmarked against traditional tree-based semantic retrieval methods.

---

## ğŸ† Results
### âœ… WANG Dataset
- ğŸ¯ Top-1 Accuracy: **87.25%**  
- ğŸ¥‡ Top-5 Accuracy: **94.38%**  
- â± Average Query Time: **0.09 seconds**

### âœ… ImageCLEF Dataset
- ğŸ¯ Top-1 Accuracy: **90.38%**  
- ğŸ“Š F1-score: **91.45%**

The proposed model outperforms existing graph-based retrieval systems in both accuracy and computational efficiency.

---

## âš ï¸ Limitations & Future Work
- ğŸ’» Requires high GPU resources for real-time inference and image generation.  
- ğŸ“‰ Full ImageCLEF evaluation is partially simulated due to hardware constraints.  
- ğŸŒ Future enhancements include:
  - Real-time user feedback integration  
  - Experiments on larger-scale datasets  
  - Development of a web-based user interface  
  - Domain-specific fine-tuning of models

---

## ğŸŒ Deployment Info
- ğŸ–¥ Implemented using a Python-based backend  
- âš¡ FAISS is used for vector indexing and fast similarity search  
- ğŸ¨ Stable Diffusion runs on CUDA-enabled GPU servers  
- ğŸŒ Can be deployed using Flask or FastAPI for web-based access


âœ¨ **Project By:**  
Krupa Chaitanya Yellamelli  
Dual Function Image System for Multimodal Interface  
